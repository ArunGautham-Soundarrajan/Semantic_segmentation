digraph {
	graph [size="136.35,136.35"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	3070451927424 [label="
 (14, 23, 128, 128)" fillcolor=darkolivegreen1]
	3067951234400 [label=AddBackward0]
	3067951233104 -> 3067951234400
	3067951233104 [label=CudnnConvolutionBackward]
	3067951234304 -> 3067951233104
	3067951234304 [label=ReluBackward1]
	3067951234544 -> 3067951234304
	3067951234544 [label=CudnnBatchNormBackward]
	3067951234640 -> 3067951234544
	3067951234640 [label=CudnnConvolutionBackward]
	3067951234736 -> 3067951234640
	3067951234736 [label=ReluBackward1]
	3067951234784 -> 3067951234736
	3067951234784 [label=CudnnBatchNormBackward]
	3067951234832 -> 3067951234784
	3067951234832 [label=CudnnConvolutionBackward]
	3067951231568 -> 3067951234832
	3067951231568 [label=UpsampleNearest2DBackward1]
	3067951233200 -> 3067951231568
	3067951233200 [label=ReluBackward1]
	3067951231520 -> 3067951233200
	3067951231520 [label=CudnnBatchNormBackward]
	3070518137520 -> 3067951231520
	3070518137520 [label=CudnnConvolutionBackward]
	3070518139584 -> 3070518137520
	3070518139584 [label=ReluBackward1]
	3070518140448 -> 3070518139584
	3070518140448 [label=CudnnBatchNormBackward]
	3069587541440 -> 3070518140448
	3069587541440 [label=CudnnConvolutionBackward]
	3067951345920 -> 3069587541440
	3067951345920 [label=CatBackward]
	3067951346112 -> 3067951345920
	3067951346112 [label=UpsampleNearest2DBackward1]
	3067951346256 -> 3067951346112
	3067951346256 [label=ReluBackward1]
	3067951346352 -> 3067951346256
	3067951346352 [label=CudnnBatchNormBackward]
	3067951346448 -> 3067951346352
	3067951346448 [label=CudnnConvolutionBackward]
	3067951346640 -> 3067951346448
	3067951346640 [label=ReluBackward1]
	3067951346784 -> 3067951346640
	3067951346784 [label=CudnnBatchNormBackward]
	3067951346880 -> 3067951346784
	3067951346880 [label=CudnnConvolutionBackward]
	3067951347072 -> 3067951346880
	3067951347072 [label=CatBackward]
	3067951347216 -> 3067951347072
	3067951347216 [label=UpsampleNearest2DBackward1]
	3067951347360 -> 3067951347216
	3067951347360 [label=ReluBackward1]
	3067951347456 -> 3067951347360
	3067951347456 [label=CudnnBatchNormBackward]
	3067951347552 -> 3067951347456
	3067951347552 [label=CudnnConvolutionBackward]
	3067951347744 -> 3067951347552
	3067951347744 [label=ReluBackward1]
	3070451786176 -> 3067951347744
	3070451786176 [label=CudnnBatchNormBackward]
	3067951232288 -> 3070451786176
	3067951232288 [label=CudnnConvolutionBackward]
	3070254787024 -> 3067951232288
	3070254787024 [label=CatBackward]
	3066686384064 -> 3070254787024
	3066686384064 [label=UpsampleNearest2DBackward1]
	3067951347888 -> 3066686384064
	3067951347888 [label=ReluBackward1]
	3067951347984 -> 3067951347888
	3067951347984 [label=CudnnBatchNormBackward]
	3067951348080 -> 3067951347984
	3067951348080 [label=CudnnConvolutionBackward]
	3067951348272 -> 3067951348080
	3067951348272 [label=ReluBackward1]
	3067951348416 -> 3067951348272
	3067951348416 [label=CudnnBatchNormBackward]
	3067951348512 -> 3067951348416
	3067951348512 [label=CudnnConvolutionBackward]
	3067951348704 -> 3067951348512
	3067951348704 [label=CatBackward]
	3067951348848 -> 3067951348704
	3067951348848 [label=UpsampleNearest2DBackward1]
	3067951348992 -> 3067951348848
	3067951348992 [label=ReluBackward1]
	3067951349088 -> 3067951348992
	3067951349088 [label=AddBackward0]
	3067951349184 -> 3067951349088
	3067951349184 [label=CudnnBatchNormBackward]
	3067951349328 -> 3067951349184
	3067951349328 [label=CudnnConvolutionBackward]
	3067951349520 -> 3067951349328
	3067951349520 [label=ReluBackward1]
	3067951349664 -> 3067951349520
	3067951349664 [label=CudnnBatchNormBackward]
	3067951349712 -> 3067951349664
	3067951349712 [label=CudnnConvolutionBackward]
	3067951349136 -> 3067951349712
	3067951349136 [label=ReluBackward1]
	3067951354208 -> 3067951349136
	3067951354208 [label=AddBackward0]
	3067951354304 -> 3067951354208
	3067951354304 [label=CudnnBatchNormBackward]
	3067951354448 -> 3067951354304
	3067951354448 [label=CudnnConvolutionBackward]
	3067951354640 -> 3067951354448
	3067951354640 [label=ReluBackward1]
	3067951354784 -> 3067951354640
	3067951354784 [label=CudnnBatchNormBackward]
	3067951354880 -> 3067951354784
	3067951354880 [label=CudnnConvolutionBackward]
	3067951354256 -> 3067951354880
	3067951354256 [label=ReluBackward1]
	3067951355168 -> 3067951354256
	3067951355168 [label=AddBackward0]
	3067951355264 -> 3067951355168
	3067951355264 [label=CudnnBatchNormBackward]
	3067951355408 -> 3067951355264
	3067951355408 [label=CudnnConvolutionBackward]
	3067951355600 -> 3067951355408
	3067951355600 [label=ReluBackward1]
	3067951355744 -> 3067951355600
	3067951355744 [label=CudnnBatchNormBackward]
	3067951355840 -> 3067951355744
	3067951355840 [label=CudnnConvolutionBackward]
	3067951348800 -> 3067951355840
	3067951348800 [label=ReluBackward1]
	3067951356128 -> 3067951348800
	3067951356128 [label=AddBackward0]
	3067951356224 -> 3067951356128
	3067951356224 [label=CudnnBatchNormBackward]
	3067951356368 -> 3067951356224
	3067951356368 [label=CudnnConvolutionBackward]
	3067951356560 -> 3067951356368
	3067951356560 [label=ReluBackward1]
	3067951356704 -> 3067951356560
	3067951356704 [label=CudnnBatchNormBackward]
	3067951356800 -> 3067951356704
	3067951356800 [label=CudnnConvolutionBackward]
	3067951356176 -> 3067951356800
	3067951356176 [label=ReluBackward1]
	3067951357088 -> 3067951356176
	3067951357088 [label=AddBackward0]
	3067951357184 -> 3067951357088
	3067951357184 [label=CudnnBatchNormBackward]
	3067951357328 -> 3067951357184
	3067951357328 [label=CudnnConvolutionBackward]
	3067951357520 -> 3067951357328
	3067951357520 [label=ReluBackward1]
	3067951357664 -> 3067951357520
	3067951357664 [label=CudnnBatchNormBackward]
	3067951357760 -> 3067951357664
	3067951357760 [label=CudnnConvolutionBackward]
	3067951357136 -> 3067951357760
	3067951357136 [label=ReluBackward1]
	3067951362208 -> 3067951357136
	3067951362208 [label=AddBackward0]
	3067951362304 -> 3067951362208
	3067951362304 [label=CudnnBatchNormBackward]
	3067951362448 -> 3067951362304
	3067951362448 [label=CudnnConvolutionBackward]
	3067951362640 -> 3067951362448
	3067951362640 [label=ReluBackward1]
	3067951362784 -> 3067951362640
	3067951362784 [label=CudnnBatchNormBackward]
	3067951362880 -> 3067951362784
	3067951362880 [label=CudnnConvolutionBackward]
	3067951362256 -> 3067951362880
	3067951362256 [label=ReluBackward1]
	3067951363168 -> 3067951362256
	3067951363168 [label=AddBackward0]
	3067951363264 -> 3067951363168
	3067951363264 [label=CudnnBatchNormBackward]
	3067951363408 -> 3067951363264
	3067951363408 [label=CudnnConvolutionBackward]
	3067951363600 -> 3067951363408
	3067951363600 [label=ReluBackward1]
	3067951363744 -> 3067951363600
	3067951363744 [label=CudnnBatchNormBackward]
	3067951363840 -> 3067951363744
	3067951363840 [label=CudnnConvolutionBackward]
	3067951363216 -> 3067951363840
	3067951363216 [label=ReluBackward1]
	3067951364128 -> 3067951363216
	3067951364128 [label=AddBackward0]
	3067951364224 -> 3067951364128
	3067951364224 [label=CudnnBatchNormBackward]
	3067951364368 -> 3067951364224
	3067951364368 [label=CudnnConvolutionBackward]
	3067951364560 -> 3067951364368
	3067951364560 [label=ReluBackward1]
	3067951364704 -> 3067951364560
	3067951364704 [label=CudnnBatchNormBackward]
	3067951364800 -> 3067951364704
	3067951364800 [label=CudnnConvolutionBackward]
	3067951364176 -> 3067951364800
	3067951364176 [label=ReluBackward1]
	3067951365088 -> 3067951364176
	3067951365088 [label=AddBackward0]
	3067951365184 -> 3067951365088
	3067951365184 [label=CudnnBatchNormBackward]
	3067951365328 -> 3067951365184
	3067951365328 [label=CudnnConvolutionBackward]
	3067951365520 -> 3067951365328
	3067951365520 [label=ReluBackward1]
	3067951365664 -> 3067951365520
	3067951365664 [label=CudnnBatchNormBackward]
	3067951365760 -> 3067951365664
	3067951365760 [label=CudnnConvolutionBackward]
	3070401738352 -> 3067951365760
	3070401738352 [label=ReluBackward1]
	3067951366048 -> 3070401738352
	3067951366048 [label=AddBackward0]
	3067951366096 -> 3067951366048
	3067951366096 [label=CudnnBatchNormBackward]
	3067951378640 -> 3067951366096
	3067951378640 [label=CudnnConvolutionBackward]
	3067951378832 -> 3067951378640
	3067951378832 [label=ReluBackward1]
	3067951378976 -> 3067951378832
	3067951378976 [label=CudnnBatchNormBackward]
	3067951379072 -> 3067951378976
	3067951379072 [label=CudnnConvolutionBackward]
	3067951365856 -> 3067951379072
	3067951365856 [label=ReluBackward1]
	3067951379360 -> 3067951365856
	3067951379360 [label=AddBackward0]
	3067951379456 -> 3067951379360
	3067951379456 [label=CudnnBatchNormBackward]
	3067951379600 -> 3067951379456
	3067951379600 [label=CudnnConvolutionBackward]
	3067951379792 -> 3067951379600
	3067951379792 [label=ReluBackward1]
	3067951379936 -> 3067951379792
	3067951379936 [label=CudnnBatchNormBackward]
	3067951380032 -> 3067951379936
	3067951380032 [label=CudnnConvolutionBackward]
	3067951379408 -> 3067951380032
	3067951379408 [label=ReluBackward1]
	3067951380320 -> 3067951379408
	3067951380320 [label=AddBackward0]
	3067951380416 -> 3067951380320
	3067951380416 [label=CudnnBatchNormBackward]
	3067951380560 -> 3067951380416
	3067951380560 [label=CudnnConvolutionBackward]
	3067951380752 -> 3067951380560
	3067951380752 [label=ReluBackward1]
	3067951380896 -> 3067951380752
	3067951380896 [label=CudnnBatchNormBackward]
	3067951380992 -> 3067951380896
	3067951380992 [label=CudnnConvolutionBackward]
	3067951380368 -> 3067951380992
	3067951380368 [label=ReluBackward1]
	3067951381280 -> 3067951380368
	3067951381280 [label=AddBackward0]
	3067951381376 -> 3067951381280
	3067951381376 [label=CudnnBatchNormBackward]
	3067951381520 -> 3067951381376
	3067951381520 [label=CudnnConvolutionBackward]
	3067951381712 -> 3067951381520
	3067951381712 [label=ReluBackward1]
	3067951381856 -> 3067951381712
	3067951381856 [label=CudnnBatchNormBackward]
	3067951381952 -> 3067951381856
	3067951381952 [label=CudnnConvolutionBackward]
	3067951347168 -> 3067951381952
	3067951347168 [label=ReluBackward1]
	3067951382240 -> 3067951347168
	3067951382240 [label=AddBackward0]
	3067951382336 -> 3067951382240
	3067951382336 [label=CudnnBatchNormBackward]
	3067951382480 -> 3067951382336
	3067951382480 [label=CudnnConvolutionBackward]
	3067969913040 -> 3067951382480
	3067969913040 [label=ReluBackward1]
	3067969913184 -> 3067969913040
	3067969913184 [label=CudnnBatchNormBackward]
	3067969913280 -> 3067969913184
	3067969913280 [label=CudnnConvolutionBackward]
	3067951382288 -> 3067969913280
	3067951382288 [label=ReluBackward1]
	3067969913568 -> 3067951382288
	3067969913568 [label=AddBackward0]
	3067969913664 -> 3067969913568
	3067969913664 [label=CudnnBatchNormBackward]
	3067969913808 -> 3067969913664
	3067969913808 [label=CudnnConvolutionBackward]
	3067969914000 -> 3067969913808
	3067969914000 [label=ReluBackward1]
	3067969914144 -> 3067969914000
	3067969914144 [label=CudnnBatchNormBackward]
	3067969914240 -> 3067969914144
	3067969914240 [label=CudnnConvolutionBackward]
	3067969913616 -> 3067969914240
	3067969913616 [label=ReluBackward1]
	3067969914528 -> 3067969913616
	3067969914528 [label=AddBackward0]
	3067969914624 -> 3067969914528
	3067969914624 [label=CudnnBatchNormBackward]
	3067969914768 -> 3067969914624
	3067969914768 [label=CudnnConvolutionBackward]
	3067969914960 -> 3067969914768
	3067969914960 [label=ReluBackward1]
	3067969915104 -> 3067969914960
	3067969915104 [label=CudnnBatchNormBackward]
	3067969915200 -> 3067969915104
	3067969915200 [label=CudnnConvolutionBackward]
	3067969914576 -> 3067969915200
	3067969914576 [label=MaxPool2DWithIndicesBackward]
	3067951346064 -> 3067969914576
	3067951346064 [label=ReluBackward1]
	3067969915536 -> 3067951346064
	3067969915536 [label=CudnnBatchNormBackward]
	3067969915632 -> 3067969915536
	3067969915632 [label=CudnnConvolutionBackward]
	3067969915824 -> 3067969915632
	3070029060352 [label="encoder.conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	3070029060352 -> 3067969915824
	3067969915824 [label=AccumulateGrad]
	3067969915584 -> 3067969915536
	3067970036928 [label="encoder.bn1.weight
 (64)" fillcolor=lightblue]
	3067970036928 -> 3067969915584
	3067969915584 [label=AccumulateGrad]
	3067969915440 -> 3067969915536
	3067970039296 [label="encoder.bn1.bias
 (64)" fillcolor=lightblue]
	3067970039296 -> 3067969915440
	3067969915440 [label=AccumulateGrad]
	3067969915392 -> 3067969915200
	3067970036480 [label="encoder.layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	3067970036480 -> 3067969915392
	3067969915392 [label=AccumulateGrad]
	3067969915152 -> 3067969915104
	3067970037632 [label="encoder.layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	3067970037632 -> 3067969915152
	3067969915152 [label=AccumulateGrad]
	3067969915008 -> 3067969915104
	3067970039424 [label="encoder.layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	3067970039424 -> 3067969915008
	3067969915008 [label=AccumulateGrad]
	3067969914912 -> 3067969914768
	3067970039744 [label="encoder.layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	3067970039744 -> 3067969914912
	3067969914912 [label=AccumulateGrad]
	3067969914720 -> 3067969914624
	3067969873856 [label="encoder.layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	3067969873856 -> 3067969914720
	3067969914720 [label=AccumulateGrad]
	3067969914672 -> 3067969914624
	3067969875712 [label="encoder.layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	3067969875712 -> 3067969914672
	3067969914672 [label=AccumulateGrad]
	3067969914576 -> 3067969914528
	3067969914432 -> 3067969914240
	3067969873664 [label="encoder.layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	3067969873664 -> 3067969914432
	3067969914432 [label=AccumulateGrad]
	3067969914192 -> 3067969914144
	3067969922688 [label="encoder.layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	3067969922688 -> 3067969914192
	3067969914192 [label=AccumulateGrad]
	3067969914048 -> 3067969914144
	3067969923648 [label="encoder.layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	3067969923648 -> 3067969914048
	3067969914048 [label=AccumulateGrad]
	3067969913952 -> 3067969913808
	3067969921600 [label="encoder.layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	3067969921600 -> 3067969913952
	3067969913952 [label=AccumulateGrad]
	3067969913760 -> 3067969913664
	3070029051968 [label="encoder.layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	3070029051968 -> 3067969913760
	3067969913760 [label=AccumulateGrad]
	3067969913712 -> 3067969913664
	3070029049920 [label="encoder.layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	3070029049920 -> 3067969913712
	3067969913712 [label=AccumulateGrad]
	3067969913616 -> 3067969913568
	3067969913472 -> 3067969913280
	3070029053248 [label="encoder.layer1.2.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	3070029053248 -> 3067969913472
	3067969913472 [label=AccumulateGrad]
	3067969913232 -> 3067969913184
	3067949373056 [label="encoder.layer1.2.bn1.weight
 (64)" fillcolor=lightblue]
	3067949373056 -> 3067969913232
	3067969913232 [label=AccumulateGrad]
	3067969913088 -> 3067969913184
	3067949372416 [label="encoder.layer1.2.bn1.bias
 (64)" fillcolor=lightblue]
	3067949372416 -> 3067969913088
	3067969913088 [label=AccumulateGrad]
	3067969912992 -> 3067951382480
	3067949374528 [label="encoder.layer1.2.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	3067949374528 -> 3067969912992
	3067969912992 [label=AccumulateGrad]
	3067951382432 -> 3067951382336
	3067949462592 [label="encoder.layer1.2.bn2.weight
 (64)" fillcolor=lightblue]
	3067949462592 -> 3067951382432
	3067951382432 [label=AccumulateGrad]
	3067951382384 -> 3067951382336
	3067949465536 [label="encoder.layer1.2.bn2.bias
 (64)" fillcolor=lightblue]
	3067949465536 -> 3067951382384
	3067951382384 [label=AccumulateGrad]
	3067951382288 -> 3067951382240
	3067951382144 -> 3067951381952
	3070028897984 [label="encoder.layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	3070028897984 -> 3067951382144
	3067951382144 [label=AccumulateGrad]
	3067951381904 -> 3067951381856
	3070028896192 [label="encoder.layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	3070028896192 -> 3067951381904
	3067951381904 [label=AccumulateGrad]
	3067951381760 -> 3067951381856
	3070028896448 [label="encoder.layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	3070028896448 -> 3067951381760
	3067951381760 [label=AccumulateGrad]
	3067951381664 -> 3067951381520
	3070028895360 [label="encoder.layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	3070028895360 -> 3067951381664
	3067951381664 [label=AccumulateGrad]
	3067951381472 -> 3067951381376
	3067970202368 [label="encoder.layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	3067970202368 -> 3067951381472
	3067951381472 [label=AccumulateGrad]
	3067951381424 -> 3067951381376
	3067970202304 [label="encoder.layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	3067970202304 -> 3067951381424
	3067951381424 [label=AccumulateGrad]
	3067951381328 -> 3067951381280
	3067951381328 [label=CudnnBatchNormBackward]
	3067951382096 -> 3067951381328
	3067951382096 [label=CudnnConvolutionBackward]
	3067951347168 -> 3067951382096
	3067951382192 -> 3067951382096
	3067949465472 [label="encoder.layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	3067949465472 -> 3067951382192
	3067951382192 [label=AccumulateGrad]
	3067951381616 -> 3067951381328
	3067949465152 [label="encoder.layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	3067949465152 -> 3067951381616
	3067951381616 [label=AccumulateGrad]
	3067951381568 -> 3067951381328
	3067949463296 [label="encoder.layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	3067949463296 -> 3067951381568
	3067951381568 [label=AccumulateGrad]
	3067951381184 -> 3067951380992
	3067970202752 [label="encoder.layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	3067970202752 -> 3067951381184
	3067951381184 [label=AccumulateGrad]
	3067951380944 -> 3067951380896
	3067970200192 [label="encoder.layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	3067970200192 -> 3067951380944
	3067951380944 [label=AccumulateGrad]
	3067951380800 -> 3067951380896
	3067970075840 [label="encoder.layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	3067970075840 -> 3067951380800
	3067951380800 [label=AccumulateGrad]
	3067951380704 -> 3067951380560
	3067970203200 [label="encoder.layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	3067970203200 -> 3067951380704
	3067951380704 [label=AccumulateGrad]
	3067951380512 -> 3067951380416
	3067970075584 [label="encoder.layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	3067970075584 -> 3067951380512
	3067951380512 [label=AccumulateGrad]
	3067951380464 -> 3067951380416
	3067970072768 [label="encoder.layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	3067970072768 -> 3067951380464
	3067951380464 [label=AccumulateGrad]
	3067951380368 -> 3067951380320
	3067951380224 -> 3067951380032
	3067970075968 [label="encoder.layer2.2.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	3067970075968 -> 3067951380224
	3067951380224 [label=AccumulateGrad]
	3067951379984 -> 3067951379936
	3067970074048 [label="encoder.layer2.2.bn1.weight
 (128)" fillcolor=lightblue]
	3067970074048 -> 3067951379984
	3067951379984 [label=AccumulateGrad]
	3067951379840 -> 3067951379936
	3067970074240 [label="encoder.layer2.2.bn1.bias
 (128)" fillcolor=lightblue]
	3067970074240 -> 3067951379840
	3067951379840 [label=AccumulateGrad]
	3067951379744 -> 3067951379600
	3067970075520 [label="encoder.layer2.2.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	3067970075520 -> 3067951379744
	3067951379744 [label=AccumulateGrad]
	3067951379552 -> 3067951379456
	3070029044544 [label="encoder.layer2.2.bn2.weight
 (128)" fillcolor=lightblue]
	3070029044544 -> 3067951379552
	3067951379552 [label=AccumulateGrad]
	3067951379504 -> 3067951379456
	3070029044480 [label="encoder.layer2.2.bn2.bias
 (128)" fillcolor=lightblue]
	3070029044480 -> 3067951379504
	3067951379504 [label=AccumulateGrad]
	3067951379408 -> 3067951379360
	3067951379264 -> 3067951379072
	3070029043840 [label="encoder.layer2.3.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	3070029043840 -> 3067951379264
	3067951379264 [label=AccumulateGrad]
	3067951379024 -> 3067951378976
	3070029043776 [label="encoder.layer2.3.bn1.weight
 (128)" fillcolor=lightblue]
	3070029043776 -> 3067951379024
	3067951379024 [label=AccumulateGrad]
	3067951378880 -> 3067951378976
	3070029044736 [label="encoder.layer2.3.bn1.bias
 (128)" fillcolor=lightblue]
	3070029044736 -> 3067951378880
	3067951378880 [label=AccumulateGrad]
	3067951378784 -> 3067951378640
	3070029044864 [label="encoder.layer2.3.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	3070029044864 -> 3067951378784
	3067951378784 [label=AccumulateGrad]
	3067951378592 -> 3067951366096
	3067949543104 [label="encoder.layer2.3.bn2.weight
 (128)" fillcolor=lightblue]
	3067949543104 -> 3067951378592
	3067951378592 [label=AccumulateGrad]
	3067951378544 -> 3067951366096
	3067949539904 [label="encoder.layer2.3.bn2.bias
 (128)" fillcolor=lightblue]
	3067949539904 -> 3067951378544
	3067951378544 [label=AccumulateGrad]
	3067951365856 -> 3067951366048
	3067951365952 -> 3067951365760
	3067949541952 [label="encoder.layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	3067949541952 -> 3067951365952
	3067951365952 [label=AccumulateGrad]
	3067951365712 -> 3067951365664
	3067949540800 [label="encoder.layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	3067949540800 -> 3067951365712
	3067951365712 [label=AccumulateGrad]
	3067951365568 -> 3067951365664
	3070028925184 [label="encoder.layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	3070028925184 -> 3067951365568
	3067951365568 [label=AccumulateGrad]
	3067951365472 -> 3067951365328
	3067949542528 [label="encoder.layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	3067949542528 -> 3067951365472
	3067951365472 [label=AccumulateGrad]
	3067951365280 -> 3067951365184
	3070028924288 [label="encoder.layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	3070028924288 -> 3067951365280
	3067951365280 [label=AccumulateGrad]
	3067951365232 -> 3067951365184
	3070028923136 [label="encoder.layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	3070028923136 -> 3067951365232
	3067951365232 [label=AccumulateGrad]
	3067951365136 -> 3067951365088
	3067951365136 [label=CudnnBatchNormBackward]
	3067951365904 -> 3067951365136
	3067951365904 [label=CudnnConvolutionBackward]
	3070401738352 -> 3067951365904
	3067951366000 -> 3067951365904
	3070028897088 [label="encoder.layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	3070028897088 -> 3067951366000
	3067951366000 [label=AccumulateGrad]
	3067951365424 -> 3067951365136
	3067949540544 [label="encoder.layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	3067949540544 -> 3067951365424
	3067951365424 [label=AccumulateGrad]
	3067951365376 -> 3067951365136
	3067949541312 [label="encoder.layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	3067949541312 -> 3067951365376
	3067951365376 [label=AccumulateGrad]
	3067951364992 -> 3067951364800
	3070028925120 [label="encoder.layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	3070028925120 -> 3067951364992
	3067951364992 [label=AccumulateGrad]
	3067951364752 -> 3067951364704
	3070028926784 [label="encoder.layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	3070028926784 -> 3067951364752
	3067951364752 [label=AccumulateGrad]
	3067951364608 -> 3067951364704
	3070028925888 [label="encoder.layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	3070028925888 -> 3067951364608
	3067951364608 [label=AccumulateGrad]
	3067951364512 -> 3067951364368
	3070028924928 [label="encoder.layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	3070028924928 -> 3067951364512
	3067951364512 [label=AccumulateGrad]
	3067951364320 -> 3067951364224
	3070028924352 [label="encoder.layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	3070028924352 -> 3067951364320
	3067951364320 [label=AccumulateGrad]
	3067951364272 -> 3067951364224
	3070028926080 [label="encoder.layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	3070028926080 -> 3067951364272
	3067951364272 [label=AccumulateGrad]
	3067951364176 -> 3067951364128
	3067951364032 -> 3067951363840
	3070028922944 [label="encoder.layer3.2.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	3070028922944 -> 3067951364032
	3067951364032 [label=AccumulateGrad]
	3067951363792 -> 3067951363744
	3067969910912 [label="encoder.layer3.2.bn1.weight
 (256)" fillcolor=lightblue]
	3067969910912 -> 3067951363792
	3067951363792 [label=AccumulateGrad]
	3067951363648 -> 3067951363744
	3067969911552 [label="encoder.layer3.2.bn1.bias
 (256)" fillcolor=lightblue]
	3067969911552 -> 3067951363648
	3067951363648 [label=AccumulateGrad]
	3067951363552 -> 3067951363408
	3067969911744 [label="encoder.layer3.2.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	3067969911744 -> 3067951363552
	3067951363552 [label=AccumulateGrad]
	3067951363360 -> 3067951363264
	3070028936960 [label="encoder.layer3.2.bn2.weight
 (256)" fillcolor=lightblue]
	3070028936960 -> 3067951363360
	3067951363360 [label=AccumulateGrad]
	3067951363312 -> 3067951363264
	3070028936832 [label="encoder.layer3.2.bn2.bias
 (256)" fillcolor=lightblue]
	3070028936832 -> 3067951363312
	3067951363312 [label=AccumulateGrad]
	3067951363216 -> 3067951363168
	3067951363072 -> 3067951362880
	3070028938752 [label="encoder.layer3.3.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	3070028938752 -> 3067951363072
	3067951363072 [label=AccumulateGrad]
	3067951362832 -> 3067951362784
	3070028936256 [label="encoder.layer3.3.bn1.weight
 (256)" fillcolor=lightblue]
	3070028936256 -> 3067951362832
	3067951362832 [label=AccumulateGrad]
	3067951362688 -> 3067951362784
	3070028936512 [label="encoder.layer3.3.bn1.bias
 (256)" fillcolor=lightblue]
	3070028936512 -> 3067951362688
	3067951362688 [label=AccumulateGrad]
	3067951362592 -> 3067951362448
	3070028936064 [label="encoder.layer3.3.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	3070028936064 -> 3067951362592
	3067951362592 [label=AccumulateGrad]
	3067951362400 -> 3067951362304
	3070028937856 [label="encoder.layer3.3.bn2.weight
 (256)" fillcolor=lightblue]
	3070028937856 -> 3067951362400
	3067951362400 [label=AccumulateGrad]
	3067951362352 -> 3067951362304
	3070028938240 [label="encoder.layer3.3.bn2.bias
 (256)" fillcolor=lightblue]
	3070028938240 -> 3067951362352
	3067951362352 [label=AccumulateGrad]
	3067951362256 -> 3067951362208
	3067951357904 -> 3067951357760
	3070028938496 [label="encoder.layer3.4.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	3070028938496 -> 3067951357904
	3067951357904 [label=AccumulateGrad]
	3067951357712 -> 3067951357664
	3070028854976 [label="encoder.layer3.4.bn1.weight
 (256)" fillcolor=lightblue]
	3070028854976 -> 3067951357712
	3067951357712 [label=AccumulateGrad]
	3067951357568 -> 3067951357664
	3070028856896 [label="encoder.layer3.4.bn1.bias
 (256)" fillcolor=lightblue]
	3070028856896 -> 3067951357568
	3067951357568 [label=AccumulateGrad]
	3067951357472 -> 3067951357328
	3070028937600 [label="encoder.layer3.4.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	3070028937600 -> 3067951357472
	3067951357472 [label=AccumulateGrad]
	3067951357280 -> 3067951357184
	3070028854400 [label="encoder.layer3.4.bn2.weight
 (256)" fillcolor=lightblue]
	3070028854400 -> 3067951357280
	3067951357280 [label=AccumulateGrad]
	3067951357232 -> 3067951357184
	3070028855808 [label="encoder.layer3.4.bn2.bias
 (256)" fillcolor=lightblue]
	3070028855808 -> 3067951357232
	3067951357232 [label=AccumulateGrad]
	3067951357136 -> 3067951357088
	3067951356992 -> 3067951356800
	3070028854784 [label="encoder.layer3.5.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	3070028854784 -> 3067951356992
	3067951356992 [label=AccumulateGrad]
	3067951356752 -> 3067951356704
	3070028856704 [label="encoder.layer3.5.bn1.weight
 (256)" fillcolor=lightblue]
	3070028856704 -> 3067951356752
	3067951356752 [label=AccumulateGrad]
	3067951356608 -> 3067951356704
	3070028854592 [label="encoder.layer3.5.bn1.bias
 (256)" fillcolor=lightblue]
	3070028854592 -> 3067951356608
	3067951356608 [label=AccumulateGrad]
	3067951356512 -> 3067951356368
	3070028855232 [label="encoder.layer3.5.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	3070028855232 -> 3067951356512
	3067951356512 [label=AccumulateGrad]
	3067951356320 -> 3067951356224
	3070029019776 [label="encoder.layer3.5.bn2.weight
 (256)" fillcolor=lightblue]
	3070029019776 -> 3067951356320
	3067951356320 [label=AccumulateGrad]
	3067951356272 -> 3067951356224
	3070029017856 [label="encoder.layer3.5.bn2.bias
 (256)" fillcolor=lightblue]
	3070029017856 -> 3067951356272
	3067951356272 [label=AccumulateGrad]
	3067951356176 -> 3067951356128
	3067951356032 -> 3067951355840
	3070029019008 [label="encoder.layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	3070029019008 -> 3067951356032
	3067951356032 [label=AccumulateGrad]
	3067951355792 -> 3067951355744
	3070028851200 [label="encoder.layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	3070028851200 -> 3067951355792
	3067951355792 [label=AccumulateGrad]
	3067951355648 -> 3067951355744
	3070028851584 [label="encoder.layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	3070028851584 -> 3067951355648
	3067951355648 [label=AccumulateGrad]
	3067951355552 -> 3067951355408
	3070029018112 [label="encoder.layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	3070029018112 -> 3067951355552
	3067951355552 [label=AccumulateGrad]
	3067951355360 -> 3067951355264
	3070028850752 [label="encoder.layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	3070028850752 -> 3067951355360
	3067951355360 [label=AccumulateGrad]
	3067951355312 -> 3067951355264
	3070028851072 [label="encoder.layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	3070028851072 -> 3067951355312
	3067951355312 [label=AccumulateGrad]
	3067951355216 -> 3067951355168
	3067951355216 [label=CudnnBatchNormBackward]
	3067951355984 -> 3067951355216
	3067951355984 [label=CudnnConvolutionBackward]
	3067951348800 -> 3067951355984
	3067951355936 -> 3067951355984
	3070028857024 [label="encoder.layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	3070028857024 -> 3067951355936
	3067951355936 [label=AccumulateGrad]
	3067951355504 -> 3067951355216
	3070029020288 [label="encoder.layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	3070029020288 -> 3067951355504
	3067951355504 [label=AccumulateGrad]
	3067951355456 -> 3067951355216
	3070029018240 [label="encoder.layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	3070029018240 -> 3067951355456
	3067951355456 [label=AccumulateGrad]
	3067951355072 -> 3067951354880
	3070028852224 [label="encoder.layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	3070028852224 -> 3067951355072
	3067951355072 [label=AccumulateGrad]
	3067951354832 -> 3067951354784
	3070028850688 [label="encoder.layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	3070028850688 -> 3067951354832
	3067951354832 [label=AccumulateGrad]
	3067951354688 -> 3067951354784
	3070028852096 [label="encoder.layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	3070028852096 -> 3067951354688
	3067951354688 [label=AccumulateGrad]
	3067951354592 -> 3067951354448
	3070028853120 [label="encoder.layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	3070028853120 -> 3067951354592
	3067951354592 [label=AccumulateGrad]
	3067951354400 -> 3067951354304
	3067970057408 [label="encoder.layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	3067970057408 -> 3067951354400
	3067951354400 [label=AccumulateGrad]
	3067951354352 -> 3067951354304
	3067970058432 [label="encoder.layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	3067970058432 -> 3067951354352
	3067951354352 [label=AccumulateGrad]
	3067951354256 -> 3067951354208
	3067951354112 -> 3067951349712
	3067970059584 [label="encoder.layer4.2.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	3067970059584 -> 3067951354112
	3067951354112 [label=AccumulateGrad]
	3067951349568 -> 3067951349664
	3067970058368 [label="encoder.layer4.2.bn1.weight
 (512)" fillcolor=lightblue]
	3067970058368 -> 3067951349568
	3067951349568 [label=AccumulateGrad]
	3067951353920 -> 3067951349664
	3067970059136 [label="encoder.layer4.2.bn1.bias
 (512)" fillcolor=lightblue]
	3067970059136 -> 3067951353920
	3067951353920 [label=AccumulateGrad]
	3067951349472 -> 3067951349328
	3067970058304 [label="encoder.layer4.2.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	3067970058304 -> 3067951349472
	3067951349472 [label=AccumulateGrad]
	3067951349280 -> 3067951349184
	3067970057856 [label="encoder.layer4.2.bn2.weight
 (512)" fillcolor=lightblue]
	3067970057856 -> 3067951349280
	3067951349280 [label=AccumulateGrad]
	3067951349232 -> 3067951349184
	3067970057792 [label="encoder.layer4.2.bn2.bias
 (512)" fillcolor=lightblue]
	3067970057792 -> 3067951349232
	3067951349232 [label=AccumulateGrad]
	3067951349136 -> 3067951349088
	3067951348800 -> 3067951348704
	3067951348656 -> 3067951348512
	3070028950592 [label="decoder.blocks.0.conv1.0.weight
 (256, 768, 3, 3)" fillcolor=lightblue]
	3070028950592 -> 3067951348656
	3067951348656 [label=AccumulateGrad]
	3067951348464 -> 3067951348416
	3070028949760 [label="decoder.blocks.0.conv1.1.weight
 (256)" fillcolor=lightblue]
	3070028949760 -> 3067951348464
	3067951348464 [label=AccumulateGrad]
	3067951348320 -> 3067951348416
	3070028950208 [label="decoder.blocks.0.conv1.1.bias
 (256)" fillcolor=lightblue]
	3070028950208 -> 3067951348320
	3067951348320 [label=AccumulateGrad]
	3067951348224 -> 3067951348080
	3067969873792 [label="decoder.blocks.0.conv2.0.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	3067969873792 -> 3067951348224
	3067951348224 [label=AccumulateGrad]
	3067951348032 -> 3067951347984
	3070029011456 [label="decoder.blocks.0.conv2.1.weight
 (256)" fillcolor=lightblue]
	3070029011456 -> 3067951348032
	3067951348032 [label=AccumulateGrad]
	3067951347792 -> 3067951347984
	3070029010944 [label="decoder.blocks.0.conv2.1.bias
 (256)" fillcolor=lightblue]
	3070029010944 -> 3067951347792
	3067951347792 [label=AccumulateGrad]
	3070401738352 -> 3070254787024
	3067951231424 -> 3067951232288
	3070029011200 [label="decoder.blocks.1.conv1.0.weight
 (128, 384, 3, 3)" fillcolor=lightblue]
	3070029011200 -> 3067951231424
	3067951231424 [label=AccumulateGrad]
	3067951232336 -> 3070451786176
	3070028985280 [label="decoder.blocks.1.conv1.1.weight
 (128)" fillcolor=lightblue]
	3070028985280 -> 3067951232336
	3067951232336 [label=AccumulateGrad]
	3067951232240 -> 3070451786176
	3070028987328 [label="decoder.blocks.1.conv1.1.bias
 (128)" fillcolor=lightblue]
	3070028987328 -> 3067951232240
	3067951232240 [label=AccumulateGrad]
	3067951347696 -> 3067951347552
	3070028985920 [label="decoder.blocks.1.conv2.0.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	3070028985920 -> 3067951347696
	3067951347696 [label=AccumulateGrad]
	3067951347504 -> 3067951347456
	3070028986688 [label="decoder.blocks.1.conv2.1.weight
 (128)" fillcolor=lightblue]
	3070028986688 -> 3067951347504
	3067951347504 [label=AccumulateGrad]
	3067951347264 -> 3067951347456
	3070029003968 [label="decoder.blocks.1.conv2.1.bias
 (128)" fillcolor=lightblue]
	3070029003968 -> 3067951347264
	3067951347264 [label=AccumulateGrad]
	3067951347168 -> 3067951347072
	3067951347024 -> 3067951346880
	3070029001920 [label="decoder.blocks.2.conv1.0.weight
 (64, 192, 3, 3)" fillcolor=lightblue]
	3070029001920 -> 3067951347024
	3067951347024 [label=AccumulateGrad]
	3067951346832 -> 3067951346784
	3070029002752 [label="decoder.blocks.2.conv1.1.weight
 (64)" fillcolor=lightblue]
	3070029002752 -> 3067951346832
	3067951346832 [label=AccumulateGrad]
	3067951346688 -> 3067951346784
	3070029002048 [label="decoder.blocks.2.conv1.1.bias
 (64)" fillcolor=lightblue]
	3070029002048 -> 3067951346688
	3067951346688 [label=AccumulateGrad]
	3067951346592 -> 3067951346448
	3070028830400 [label="decoder.blocks.2.conv2.0.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	3070028830400 -> 3067951346592
	3067951346592 [label=AccumulateGrad]
	3067951346400 -> 3067951346352
	3070028828992 [label="decoder.blocks.2.conv2.1.weight
 (64)" fillcolor=lightblue]
	3070028828992 -> 3067951346400
	3067951346400 [label=AccumulateGrad]
	3067951346160 -> 3067951346352
	3070028831360 [label="decoder.blocks.2.conv2.1.bias
 (64)" fillcolor=lightblue]
	3070028831360 -> 3067951346160
	3067951346160 [label=AccumulateGrad]
	3067951346064 -> 3067951345920
	3067951345824 -> 3069587541440
	3070029047424 [label="decoder.blocks.3.conv1.0.weight
 (32, 128, 3, 3)" fillcolor=lightblue]
	3070029047424 -> 3067951345824
	3067951345824 [label=AccumulateGrad]
	3070518139536 -> 3070518140448
	3070029046464 [label="decoder.blocks.3.conv1.1.weight
 (32)" fillcolor=lightblue]
	3070029046464 -> 3070518139536
	3070518139536 [label=AccumulateGrad]
	3067951345728 -> 3070518140448
	3070029046976 [label="decoder.blocks.3.conv1.1.bias
 (32)" fillcolor=lightblue]
	3070029046976 -> 3067951345728
	3067951345728 [label=AccumulateGrad]
	3070518139872 -> 3070518137520
	3070028973888 [label="decoder.blocks.3.conv2.0.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	3070028973888 -> 3070518139872
	3070518139872 [label=AccumulateGrad]
	3070518138480 -> 3067951231520
	3070028974272 [label="decoder.blocks.3.conv2.1.weight
 (32)" fillcolor=lightblue]
	3070028974272 -> 3070518138480
	3070518138480 [label=AccumulateGrad]
	3070518137376 -> 3067951231520
	3070028974208 [label="decoder.blocks.3.conv2.1.bias
 (32)" fillcolor=lightblue]
	3070028974208 -> 3070518137376
	3070518137376 [label=AccumulateGrad]
	3067951235024 -> 3067951234832
	3070028888960 [label="decoder.blocks.4.conv1.0.weight
 (16, 32, 3, 3)" fillcolor=lightblue]
	3070028888960 -> 3067951235024
	3067951235024 [label=AccumulateGrad]
	3067951234880 -> 3067951234784
	3070028888640 [label="decoder.blocks.4.conv1.1.weight
 (16)" fillcolor=lightblue]
	3070028888640 -> 3067951234880
	3067951234880 [label=AccumulateGrad]
	3067951233920 -> 3067951234784
	3070028889472 [label="decoder.blocks.4.conv1.1.bias
 (16)" fillcolor=lightblue]
	3070028889472 -> 3067951233920
	3067951233920 [label=AccumulateGrad]
	3067951233344 -> 3067951234640
	3070028971520 [label="decoder.blocks.4.conv2.0.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
	3070028971520 -> 3067951233344
	3067951233344 [label=AccumulateGrad]
	3067951234592 -> 3067951234544
	3070028969408 [label="decoder.blocks.4.conv2.1.weight
 (16)" fillcolor=lightblue]
	3070028969408 -> 3067951234592
	3067951234592 [label=AccumulateGrad]
	3067951233152 -> 3067951234544
	3070028969728 [label="decoder.blocks.4.conv2.1.bias
 (16)" fillcolor=lightblue]
	3070028969728 -> 3067951233152
	3067951233152 [label=AccumulateGrad]
	3067951232480 -> 3067951233104
	3070028841216 [label="segmentation_head.0.weight
 (23, 16, 3, 3)" fillcolor=lightblue]
	3070028841216 -> 3067951232480
	3067951232480 [label=AccumulateGrad]
	3067951234448 -> 3067951234400
	3067951234448 [label=ViewBackward]
	3070518140640 -> 3067951234448
	3070028844672 [label="segmentation_head.0.bias
 (23)" fillcolor=lightblue]
	3070028844672 -> 3070518140640
	3070518140640 [label=AccumulateGrad]
	3067951234400 -> 3070451927424
}
